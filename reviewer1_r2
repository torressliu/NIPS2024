Supplement for comparison experiments between our method and the delayed MDP methods

We enhanced the comparison experiment between the MARS and delayed MDP methods by increasing the number of seeds from 4 to 8. Furthermore, two new baselines are added to enhance the richness of the experiment.  The experimental results show that the advantage of our method is further improved as the number of seeds doubles.

*Baselines:* We append two newest baselines to the three existing SOTA delayed MDP methods (DCAC, SA, delayed Dreamer): BPQL [1], the latest Actor-critic-based continuous control algorithm for delayed feedback environments. AD-RL [2],  a SOTA method that utilizes auxiliary tasks with short delays to accelerate RL with long delays.

*Benchmarks:* We chose six difficult tasks. DeepMind Control tasks: Dog Run, Dog Trot, Dog Stand and Humanoid Walk. Besides, two robotic arm control tasks in MetaWorld: Sweep Into and Coffee Push are used. The environmental parameters and network hyperparameters remained consistent with the main experiment.

*Metrics:* We evaluate methods in terms of performance and Smoothness (whether the motion is coherent and stable, invalid jitter and stagnation will reduce the score, please refer to Sec.5.2 for details).

Table 1. Performance in newly added difficult tasks (smoothness (%)):
| Method      |Dog Run|Dog Trot| Dog Stand| Humanoid Walk|Sweep Into|Coffee Push|
| :-----------: | :-----------: | :------------: | :-----------: | :-----------: | :-----------: | :-----------: |
| **Ours**|$124.61\pm 24.71 (78)$|$592.42\pm 28.76 (88)$|$626.11\pm 18.36 (76)$|$120.82\pm 26.45 (85)$|$0.53\pm 0.04 (74)$|$0.42\pm 0.05(69)$|
| DCAC|$91.48\pm 16.75 (55)$|$411.06\pm 36.52 (76)$|$538.47\pm 22.73 (64)$|$101.63\pm 15.28 (46)$|$0.42\pm 0.08 (48)$|$0.23\pm 0.11 (51)$|
| BPQL|$95.37\pm 20.31 (62)$|$451.72\pm 36.67 (78)$|$526.13\pm 17.92 (66)$|$92.84\pm 22.51 (61)$|$0.47\pm 0.11 (53)$|$0.27\pm 0.05 (57)$|
| AD-RL|$88.26\pm 14.03 (64)$|$448.49\pm 24.72 (72)$|$471.82\pm 21.17 (51)$|$105.32\pm 15.25 (57)$|$0.39\pm 0.08 (45)$|$0.23\pm 0.07 (41)$|
| SA |$93.26\pm 28.14 (33)$|$384.26\pm 45.03 (42)$|$486.91\pm 10.51 (32)$|$78.21\pm 27.41 (47)$|$0.49\pm 0.07 (42)$|$0.33\pm 0.06 (34)$|
| delayed Dreamer |$95.28\pm 19.42 (46)$|$416.65\pm 24.18 (45)$|$526.07\pm 11.52 (38)$|$92.07\pm 13.59 (43)$|$0.48\pm 0.08 (36)$|$0.36\pm 0.03 (46)$|

Table 1 shows that our method performs better than Delayed MDP methods in almost all intermitted MDP control tasks while ensuring the smooth and coherent motion of the agent.  Besides, the advantage of our method is further improved as the number of seeds doubles.

[1]: Kim, Jangwon, et al. "Belief projection-based reinforcement learning for environments with delayed feedback."NuerIPS 2023.

[2]: Wu, Qingyuan, et al. "Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task." ICML 2024.
